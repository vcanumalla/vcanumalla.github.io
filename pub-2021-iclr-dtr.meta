<meta name="description" content="Dynamic Tensor Rematerialization (DTR), is an online algorithm for checkpointing deep learning models. DTR enables training under restricted memory budgets by freeing intermediate activations from memory and recomputing them on demand. Unlike static checkpointing techniques, DTR supports dynamic models and achieves comparable performance. For example, DTR can train an N-layer linear feedforward network on an Ω(√ N) memory budget with only O(N) tensor operations. We prototyped in PyTorch simply by interposing on tensor allocations and operator calls."

<!-- OpenGraph -->
<meta property="og:url" content="https://ztatlock.net/pub-2021-iclr-dtr.html">
<meta property="og:type" content="website">
<meta property="og:title" content="Dynamic Tensor Rematerialization">
<meta property="og:description" content="DTR is an online algorithm for checkpointing deep learning models that enables training under restricted memory budgets by freeing intermediate activations from memory and recomputing them on demand.">
<meta property="og:image" content="https://ztatlock.net/pubs/2021-iclr-dtr/2021-iclr-dtr-meta.png">

<!-- Twitter -->
<meta name="twitter:card" content="summary_large_image">
<meta property="twitter:domain" content="ztatlock.net">
<meta property="twitter:url" content="https://ztatlock.net/pub-2021-iclr-dtr.html">
<meta name="twitter:title" content="Dynamic Tensor Rematerialization">
<meta property="twitter:description" content="DTR is an online algorithm for checkpointing deep learning models that enables training under restricted memory budgets by freeing intermediate activations from memory and recomputing them on demand.">
<meta name="twitter:image" content="https://ztatlock.net/pubs/2021-iclr-dtr/2021-iclr-dtr-meta.png">
