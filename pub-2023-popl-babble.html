<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="icon" href="img/favicon.png">
  <link rel="stylesheet" href="style.css">
  <title>babble: Learning Better Abstractions with E-Graphs and Anti-Unification</title>
</head>
<body>
<section id="babble-Learning-Better-Abstractions-with-E-Graphs-and-Anti-Unification">
<h1>babble: Learning Better Abstractions with E-Graphs and Anti-Unification</h1>
<p><a href="https://cao.sh/">David Cao</a>,
&nbsp;<a href="https://github.com/rosekunkel">Rose Kunkel</a>,
&nbsp;<a href="https://cnandi.com/">Chandrakana Nandi</a>,
&nbsp;<a href="https://www.mwillsey.com/">Max Willsey</a>,
&nbsp;<a href="https://ztatlock.net/">Zachary Tatlock</a>,
&nbsp;<a href="https://cseweb.ucsd.edu/~npolikarpova/">Nadia Polikarpova</a></p>
<p>Principles of Programming Languages (POPL) 2023</p>
<div class="photo">
<p><a href="pubs/2023-popl-babble/2023-popl-babble.pdf"><img alt="babble: Learning Better Abstractions with E-Graphs and Anti-Unification" src="pubs/2023-popl-babble/2023-popl-babble-absimg.png"></a></p>
</div>
<ul class="columns columns-8rem">
<li>
<a href="https://dl.acm.org/doi/abs/10.1145/3571207">publisher</a>
</li>
<li>
<a href="pubs/2023-popl-babble/2023-popl-babble.pdf">paper</a>
</li>
<li>
<a href="https://arxiv.org/abs/2212.04596">arXiv</a>
</li>
<li>
<a href="https://www.youtube.com/watch?v=ogGiKdhDmhU">talk</a>
</li>
<li>
<a href="pubs/2023-popl-babble/2023-popl-babble.bib">bib</a>
</li>
</ul>
<section id="Abstract">
<h2>Abstract</h2>
<p>Library learning compresses a given corpus of programs by extracting common
structure from the corpus into reusable library functions. Prior work on
library learning suffers from two limitations that prevent it from scaling to
larger, more complex inputs. First, it explores too many candidate library
functions that are not useful for compression. Second, it is not robust to
syntactic variation in the input.</p>
<p>We propose library learning modulo theory (LLMT), a new library learning
algorithm that additionally takes as input an equational theory for a given
problem domain. LLMT uses e-graphs and equality saturation to compactly
represent the space of programs equivalent modulo the theory, and uses a novel
e-graph antiunification technique to find common patterns in the corpus more
directly and efficiently.</p>
<p>We implemented LLMT in a tool named babble. Our evaluation shows that babble
achieves better compression orders of magnitude faster than the state of the
art. We also provide a qualitative evaluation showing that babble learns
reusable functions on inputs previously out of reach for library learning.</p>
</section>
<section id="BibTeX">
<h2>BibTeX</h2>
<pre class="bib"><code>@article{2023-popl-babble,
  title     = {babble: Learning Better Abstractions with E-Graphs and Anti-Unification},
  author    = {David Cao and Rose Kunkel and Chandrakana Nandi and Max Willsey and Zachary Tatlock and Nadia Polikarpova},
  journal   = {Proceedings of the ACM on Programming Languages},
  number    = {POPL},
  year      = {2023},
  publisher = {Association for Computing Machinery},
  url       = {https://doi.org/10.1145/3571207},
  doi       = {10.1145/3571207},
}
</code></pre>
<p><a href="publications.html">üìù publications index</a></p>
</section>
</section>
</body>
</html>
