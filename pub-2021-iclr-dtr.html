<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="icon" href="img/favicon.png">
  <link rel="stylesheet" href="style.css">
  <title>Dynamic Tensor Rematerialization</title>
  <link rel="canonical" href="https://ztatlock.net/pub-2021-iclr-dtr.html">
<meta name="description" content="Learn about Dynamic Tensor Rematerialization (DTR), an extensible and general online algorithm for checkpointing deep learning models. DTR enables training under restricted memory budgets by freeing intermediate activations from memory and recomputing them on demand. Unlike static checkpointing techniques, DTR supports dynamic models and achieves comparable performance. Discover how DTR can train an N-layer linear feedforward network on an ‚Ñ¶(‚àö N) memory budget with only O(N) tensor operations. Explore the DTR prototype in PyTorch, which seamlessly integrates with tensor allocations and operator calls, collecting lightweight metadata on tensors for efficient training.">

<!-- OpenGraph -->
<meta property="og:url" content="https://ztatlock.net/pub-2021-iclr-dtr.html">
<meta property="og:type" content="website">
<meta property="og:title" content="Dynamic Tensor Rematerialization">
<meta property="og:description" content="Dynamic Tensor Rematerialization (DTR) is an online algorithm for checkpointing deep learning models. DTR enables training under restricted memory budgets by freeing intermediate activations from memory and recomputing them on demand. Discover how DTR can train an N-layer linear feedforward network on an ‚Ñ¶(‚àö N) memory budget with only O(N) tensor operations. Explore the DTR prototype in PyTorch, which seamlessly integrates with tensor allocations and operator calls, collecting lightweight metadata on tensors for efficient training.">
<meta property="og:image" content="https://ztatlock.net/pubs/2021-iclr-dtr/2021-iclr-dtr-absimg.png">

<!-- Twitter -->
<meta name="twitter:card" content="summary_large_image">
<meta property="twitter:domain" content="ztatlock.net">
<meta property="twitter:url" content="https://ztatlock.net/pub-2021-iclr-dtr.html">
<meta name="twitter:title" content="Dynamic Tensor Rematerialization">
<meta property="twitter:description" content="Dynamic Tensor Rematerialization (DTR) is an online algorithm for checkpointing deep learning models. DTR enables training under restricted memory budgets by freeing intermediate activations from memory and recomputing them on demand. Discover how DTR can train an N-layer linear feedforward network on an ‚Ñ¶(‚àö N) memory budget with only O(N) tensor operations. Explore the DTR prototype in PyTorch, which seamlessly integrates with tensor allocations and operator calls, collecting lightweight metadata on tensors for efficient training.">
<meta name="twitter:image" content="https://ztatlock.net/pubs/2021-iclr-dtr/2021-iclr-dtr-absimg.png">
</head>
<body>
<section id="Dynamic-Tensor-Rematerialization">
<h1>Dynamic Tensor Rematerialization</h1>
<p><a href="https://marisakirisame.github.io/">Marisa Kirisame</a>,
&nbsp;<a href="https://slyubomirsky.github.io/">Steven Lyubomirsky</a>,
&nbsp;<a href="https://altanh.com/">Altan Haan</a>,
&nbsp;<a href="https://jenniferbrennan.github.io/">Jennifer Brennan</a>,
&nbsp;<a href="https://only.rs/">Mike He</a>,
&nbsp;<a href="https://jroesch.github.io/">Jared Roesch</a>,
&nbsp;<a href="https://tqchen.com/">Tianqi Chen</a>,
&nbsp;<a href="https://ztatlock.net/">Zachary Tatlock</a></p>
<p>International Conference on Learning Representations (ICLR) 2021<br>
‚òÖ Spotlight Paper</p>
<div class="photo">
<p><a href="pubs/2021-iclr-dtr/2021-iclr-dtr.pdf"><img alt="Dynamic Tensor Rematerialization" src="pubs/2021-iclr-dtr/2021-iclr-dtr-absimg.png"></a></p>
</div>
<ul class="columns columns-8rem">
<li>
<a href="pubs/2021-iclr-dtr/2021-iclr-dtr.pdf">paper</a>
</li>
<li>
<a href="https://www.youtube.com/watch?v=kxlbpwBJzA4">teaser</a>
</li>
<li>
<a href="https://www.youtube.com/watch?v=S9KJ37Sx2XY">talk</a>
</li>
<li>
<a href="pubs/2021-iclr-dtr/2021-iclr-dtr-slides.pdf">slides</a>
</li>
<li>
<a href="pubs/2021-iclr-dtr/2021-iclr-dtr-poster.pdf">poster</a>
</li>
<li>
<a href="http://sampl.cs.washington.edu/projects/dtr.html">project</a>
</li>
<li>
<a href="https://github.com/uwsampl/dtr-prototype">code</a>
</li>
<li>
<a href="https://openreview.net/forum?id=Vfs_2RnOD0H">publisher</a>
</li>
<li>
<a href="https://arxiv.org/abs/2006.09616">arXiv</a>
</li>
<li>
<a href="pubs/2021-iclr-dtr/2021-iclr-dtr.bib">bib</a>
</li>
</ul>
<section id="Abstract">
<h2>Abstract</h2>
<p>Checkpointing enables the training of deep learning models under restricted
memory budgets by freeing intermediate activations from memory and recomputing
them on demand. Current checkpointing techniques statically plan these
recomputations offline and assume static computation graphs. We demonstrate
that a simple online algorithm can achieve comparable performance by
introducing Dynamic Tensor Rematerialization (DTR), a greedy online algorithm
for checkpointing that is extensible and general, is parameterized by eviction
policy, and supports dynamic models. We prove that DTR can train an N-layer
linear feedforward network on an ‚Ñ¶(‚àö N) memory budget with only O(N) tensor
operations. DTR closely matches the performance of optimal static checkpointing
in simulated experiments. We incorporate a DTR prototype into PyTorch merely by
interposing on tensor allocations and operator calls and collecting lightweight
metadata on tensors.</p>
</section>
<section id="Extra">
<h2>Extra</h2>
<p>Real Time<sup>TM</sup> vaporwave DTR by <a href="https://altanh.com/">Altan Haan</a>.</p>
<div class="photo">
<video controls playsinline>
  <source src='pubs/2021-iclr-dtr/2021-iclr-dtr-vaporwave.mp4' type='video/mp4'>
  Your browser does not support the video tag.
</video>
</div>
</section>
<section id="BibTeX">
<h2>BibTeX</h2>
<pre class="bib"><code>@inproceedings{2021-iclr-dtr,
  title     = {Dynamic Tensor Rematerialization},
  author    = {Kirisame, Marisa and Lyubomirsky, Steven and Haan, Altan and Brennan, Jennifer and He, Mike and Roesch, Jared and Chen, Tianqi and Tatlock, Zachary},
  booktitle = {International Conference on Learning Representations},
  date      = {2021},
  url       = {https://openreview.net/forum?id=Vfs_2RnOD0H},
}
</code></pre>
<p><a href="publications.html">üìù publications index</a></p>
</section>
</section>
</body>
</html>
