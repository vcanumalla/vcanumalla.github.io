<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="author" content="Zachary Tatlock">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Nimble: Efficiently Compiling Dynamic Neural Networks for Model Inference</title>
  <link rel="icon" href="img/favicon.png">
  <link rel="stylesheet" href="style.css">
  <link rel="canonical" href="https://ztatlock.net/pub-2021-mlsys-nimble.html">
<meta name="description" content="Nimble: flexible optimization, compilation, and execution of dynamic neural networks with control flow and dynamically-shaped data structures on CPU and GPU.">

<!-- OpenGraph -->
<meta property="og:url" content="https://ztatlock.net/pub-2021-mlsys-nimble.html">
<meta property="og:type" content="website">
<meta property="og:title" content="Nimble: Efficiently Compiling Dynamic Neural Networks for Model Inference">
<meta property="og:description" content="Flexible optimization, compilation, and execution of dynamic neural networks with control flow and dynamically-shaped data structures on CPU and GPU.">
<meta property="og:image" content="https://ztatlock.net/pubs/2021-mlsys-nimble/2021-mlsys-nimble-meta.png">

<!-- Twitter -->
<meta name="twitter:card" content="summary_large_image">
<meta property="twitter:domain" content="ztatlock.net">
<meta property="twitter:url" content="https://ztatlock.net/pub-2021-mlsys-nimble.html">
<meta name="twitter:title" content="Nimble: Efficiently Compiling Dynamic Neural Networks for Model Inference">
<meta name="twitter:description" content="Flexible optimization, compilation, and execution of dynamic neural networks with control flow and dynamically-shaped data structures on CPU and GPU.">
<meta name="twitter:image" content="https://ztatlock.net/pubs/2021-mlsys-nimble/2021-mlsys-nimble-meta.png">
  <script async src="https://analytics.umami.is/script.js" data-website-id="e9f6fa87-a4bb-48ac-84df-b2d190867eb9" data-domains="ztatlock.net"></script>
</head>
<body>
<section id="Nimble:-Efficiently-Compiling-Dynamic-Neural-Networks-for-Model-Inference">
<h1>Nimble: Efficiently Compiling Dynamic Neural Networks for Model Inference</h1>
<p><a href="https://shenhaichen.com/">Haichen Shen</a>,
&nbsp;<a href="https://jroesch.github.io/">Jared Roesch</a>,
&nbsp;<a href="https://www.amazon.science/author/zhi-chen">Zhi Chen</a>,
&nbsp;<a href="https://www.amazon.science/author/wei-chen">Wei Chen</a>,
&nbsp;<a href="https://www.amazon.science/author/yong-wu">Yong Wu</a>,
&nbsp;<a href="https://www.amazon.science/author/mu-li">Mu Li</a>,
&nbsp;<a href="https://www.amazon.science/author/vin-sharma">Vin Sharma</a>,
&nbsp;<a href="https://ztatlock.net/">Zachary Tatlock</a>,
&nbsp;<a href="http://yidawang.org/">Yida Wang</a></p>
<p>Conference on Machine Learning and Systems (MLSys) 2021</p>
<div class="photo">
<p><a href="pubs/2021-mlsys-nimble/2021-mlsys-nimble.pdf"><img alt="Nimble: Efficiently Compiling Dynamic Neural Networks for Model Inference" src="pubs/2021-mlsys-nimble/2021-mlsys-nimble-absimg.png"></a></p>
</div>
<ul class="columns columns-8rem">
<li>
<a href="pubs/2021-mlsys-nimble/2021-mlsys-nimble.pdf">paper</a>
</li>
<li>
<a href="https://slideslive.com/38952766/oral-nimble-efficiently-compiling-dynamic-neural-networks-for-model-inference">talk</a>
</li>
<li>
<a href="pubs/2021-mlsys-nimble/2021-mlsys-nimble-slides.pdf">slides</a>
</li>
<li>
<a href="https://tvm.apache.org/">project</a>
</li>
<li>
<a href="https://github.com/apache/tvm">code</a>
</li>
<li>
<a href="https://proceedings.mlsys.org/paper_files/paper/2021/hash/5b47430e24a5a1f9fe21f0e8eb814131-Abstract.html">publisher</a>
</li>
<li>
<a href="https://arxiv.org/abs/2006.03031">arXiv</a>
</li>
<li>
<a href="pubs/2021-mlsys-nimble/2021-mlsys-nimble.bib">bib</a>
</li>
</ul>
<section id="Abstract">
<h2>Abstract</h2>
<p>Modern deep neural networks increasingly make use of features such as control
flow, dynamic data structures, and dynamic tensor shapes. Existing deep
learning systems focus on optimizing and executing static neural networks
which assume a pre-determined model architecture and input data
shapes‚Äîassumptions that are violated by dynamic neural networks. Therefore,
executing dynamic models with deep learning systems is currently both
inflexible and sub-optimal, if not impossible. Optimizing dynamic neural
networks is more challenging than static neural networks; optimizations must
consider all possible execution paths and tensor shapes. This paper proposes
Nimble, a high-performance and flexible system to optimize, compile, and
execute dynamic neural networks on multiple platforms. Nimble handles model
dynamism by introducing a dynamic type system, a set of dynamism-oriented
optimizations, and a light-weight virtual machine runtime. Our evaluation
demonstrates that Nimble outperforms existing solutions for dynamic neural
networks by up to 20x on hardware platforms including Intel CPUs, ARM CPUs,
and Nvidia GPUs.</p>
</section>
<section id="BibTeX">
<h2>BibTeX</h2>
<pre class="bib"><code>@inproceedings{2021-mlsys-nimble,
  title     = {Nimble: Efficiently Compiling Dynamic Neural Networks for Model Inference},
  author    = {Shen, Haichen and Roesch, Jared and Chen, Zhi and Chen, Wei and Wu, Yong and Li, Mu and Sharma, Vin and Tatlock, Zachary and Wang, Yida},
  booktitle = {Proceedings of Machine Learning and Systems},
  date      = {2021},
}
</code></pre>
<p><a href="publications.html">üìù publications index</a></p>
</section>
</section>
</body>
</html>
